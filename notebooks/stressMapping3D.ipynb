{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_graph(G,node_adj_frame):\n",
    "#     G.add_nodes_from([i for i in range(len(node_adj_frame))])\n",
    "#     labels = {}\n",
    "#     labels = node_adj_frame.columns\n",
    "#     for i in range(len(node_adj_frame)):\n",
    "#         snode = -1\n",
    "#         if node_adj_frame[labels[0]][i] < 81:\n",
    "#             snode= node_adj_frame[labels[0]][i]-1 #s.no.\n",
    "#         else:\n",
    "#             snode= node_adj_frame[labels[0]][i]-2\n",
    "#         temp = node_adj_frame[labels[2]][i]\n",
    "#         if ',' in str(temp):\n",
    "#             sedge_arr = temp.split(',')\n",
    "#             for j in range(0, len(sedge_arr)):\n",
    "#                 k = int(sedge_arr[j])\n",
    "#                 if k < 81:\n",
    "#                     G.add_edge(snode, k-1)\n",
    "#                 else:\n",
    "#                     G.add_edge(snode, k-2)\n",
    "#         elif temp == np.nan:\n",
    "#             print(\"ERROR: Not found in the adjacency excel sheet\")\n",
    "#         else:\n",
    "#             G.add_edge(snode, int(temp)-1)\n",
    "#     return\n",
    "\n",
    "def init_graph(G,node_adj_frame):\n",
    "    G.add_nodes_from([i for i in range(len(node_adj_frame))])\n",
    "    labels = {}\n",
    "    labels = node_adj_frame.columns\n",
    "    for i in range(len(node_adj_frame)):\n",
    "        snode = node_adj_frame[labels[0]][i]-1\n",
    "        if snode == 80:\n",
    "            continue\n",
    "        temp = node_adj_frame[labels[2]][i]\n",
    "        if ',' in str(temp):\n",
    "            sedge_arr = temp.split(',')\n",
    "            for j in range(0, len(sedge_arr)):\n",
    "                k = int(sedge_arr[j])\n",
    "                G.add_edge(snode, k-1)\n",
    "        elif np.isnan(temp):\n",
    "            print(\"ERROR: Not found in the adjacency excel sheet\")\n",
    "        else:\n",
    "            G.add_edge(snode, int(temp)-1)\n",
    "    G.remove_node(80)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_graph_attr(G,AdjFile,df,col1,col2, col3):\n",
    "#     node_adj_frame = pd.read_excel(AdjFile)\n",
    "#     node_list = node_adj_frame[\"KGISTalukN\"].tolist()\n",
    "#     nodeAttr = {}\n",
    "#     init_graph(G,node_adj_frame)\n",
    "#     capability_vector = list(zip(df[col1],df[col2],df[col3]))\n",
    "#     node_attri_dict = dict(zip(df[\"Taluka\"],capability_vector))\n",
    "#     node_attri_dict = dict((k.lower(), v) for k, v in node_attri_dict.items())\n",
    "#     for i in range(len(node_adj_frame)):\n",
    "#         temp = {}\n",
    "#         temp[\"capabilityvector\"] = node_attri_dict[node_list[i].lower()]\n",
    "#         temp[\"nodeStress\"] = 0\n",
    "#         temp[\"name\"] = node_list[i]\n",
    "#         nodeAttr[i] = temp\n",
    "#     nx.set_node_attributes(G, nodeAttr)\n",
    "\n",
    "def init_graph_attr(G,AdjFile,df,col1, col2, col3):\n",
    "    node_adj_frame = pd.read_excel(AdjFile)\n",
    "    node_list = node_adj_frame[\"KGISTalukN\"].tolist()\n",
    "    node_list.insert(80, \"\")\n",
    "    nodeAttr = {}\n",
    "    init_graph(G,node_adj_frame)\n",
    "    capability_vector = list(zip(df[col1], df[col2], df[col3]))\n",
    "    node_attri_dict = dict(zip(df[\"Taluka\"],capability_vector))\n",
    "    node_attri_dict = dict((k.lower(), v) for k, v in node_attri_dict.items())\n",
    "    for i in range(len(node_adj_frame)):\n",
    "        temp = {}\n",
    "        if i == 80:\n",
    "            continue\n",
    "        temp[\"capabilityvector\"] = node_attri_dict[node_list[i].lower()]\n",
    "        temp[\"nodeStress\"] = 0\n",
    "        temp[\"name\"] = node_list[i]\n",
    "        nodeAttr[i] = temp\n",
    "    nt = {}\n",
    "    nt[\"capabilityvector\"] = node_attri_dict[node_list[226].lower()]\n",
    "    nt[\"nodeStress\"] = 0\n",
    "    nt[\"name\"] = \"Hadagali\"\n",
    "    nodeAttr[226] = nt\n",
    "    nx.set_node_attributes(G, nodeAttr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "df = pd.read_excel('../input_files/Combined3D.xlsx')\n",
    "# df = pd.read_excel('Combined3D.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_taluka_dict = defaultdict(list)\n",
    "for k, v in zip(df[\"District_GIS\"], df[\"Taluka\"]):\n",
    "    dist_taluka_dict[k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addList(l1,l2):\n",
    "    for i in range(len(l1)):\n",
    "        l1[i] = l1[i] + l2[i]\n",
    "    return l1\n",
    "def divList(l1,k):\n",
    "    for i in range(len(l1)):\n",
    "        l1[i] = l1[i]/k\n",
    "    return l1\n",
    "def l2_normalization(l1,l2):\n",
    "    k = 0\n",
    "    for i in range(len(l1)):\n",
    "        k+= (l1[i] - l2[i])**2\n",
    "    return math.sqrt(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_stress(G):\n",
    "    taluka_stress_dict = {}\n",
    "    for n in G.nodes():\n",
    "        centroid = [0,0,0]\n",
    "        neighList = list(G.neighbors(n))\n",
    "        for nei in neighList:\n",
    "            try:\n",
    "                centroid = addList(centroid,list(G.nodes[nei][\"capabilityvector\"]))\n",
    "            except(KeyError):\n",
    "                pass\n",
    "        try:\n",
    "            # divide by 3?\n",
    "            G.nodes[n][\"nodeStress\"] = l2_normalization(divList(centroid,len(neighList)),list(G.nodes[n][\"capabilityvector\"]))\n",
    "        except(KeyError):\n",
    "            pass\n",
    "        try:\n",
    "            taluka_stress_dict[G.nodes[n][\"name\"].lower()]=G.nodes[n][\"nodeStress\"]\n",
    "        except(KeyError):\n",
    "            pass\n",
    "    return taluka_stress_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_stability(G):\n",
    "    taluka_stress_dict = {}\n",
    "    for n in G.nodes():\n",
    "        centroid = [0,0,0]\n",
    "        neighList = list(G.neighbors(n))\n",
    "        for nei in neighList:\n",
    "            try:\n",
    "                centroid = addList(centroid,list(G.nodes[nei][\"capabilityvector\"]))\n",
    "            except(KeyError):\n",
    "                pass\n",
    "        try:\n",
    "            # divide by 3?\n",
    "            G.nodes[n][\"nodeStress\"] = 1 - l2_normalization(divList(centroid,len(neighList)),list(G.nodes[n][\"capabilityvector\"]))\n",
    "        except(KeyError):\n",
    "            pass\n",
    "        try:\n",
    "            taluka_stress_dict[G.nodes[n][\"name\"].lower()]=G.nodes[n][\"nodeStress\"]\n",
    "        except(KeyError):\n",
    "            pass\n",
    "    return taluka_stress_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Taluka'] = df['Taluka'].apply(str.lower)\n",
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized MMR\", \"Normalized IMR\", \"Normalized PAW\")\n",
    "initialstress = get_node_stress(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Initial Stress\"] = df[\"Taluka\"].map(initialstress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized IMR (Kuchha Houses-20%)\", \"Normalized MMR (HM -20%)\", \"Normalized PAW (HM-20%)\")\n",
    "KHminus20stress = get_node_stress(G)\n",
    "df[\"Stress(KH - 20%)\"] = df[\"Taluka\"].map(KHminus20stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized IMR (Kuchha Houses-10%)\", \"Normalized MMR (HM -10%)\", \"Normalized PAW (HM-10%)\")\n",
    "KHminus10stress = get_node_stress(G)\n",
    "df[\"Stress(KH - 10%)\"] = df[\"Taluka\"].map(KHminus10stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized IMR (Kuchha Houses+10%)\", \"Normalized MMR (HM +10%)\", \"Normalized PAW (HM+10%)\")\n",
    "KHplus10stress = get_node_stress(G)\n",
    "df[\"Stress(KH + 10%)\"] = df[\"Taluka\"].map(KHplus10stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized IMR (Kuchha Houses+20%)\", \"Normalized MMR (HM +20%)\", \"Normalized PAW (HM+20%)\")\n",
    "KHplus20stress = get_node_stress(G)\n",
    "df[\"Stress(KH + 20%)\"] = df[\"Taluka\"].map(KHplus20stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_excel(\"3D_StabilityFile.xlsx\")\n",
    "df.to_excel('../windows_3d/3D_taluka_stress.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(taluka_stress_dict):\n",
    "    dist_stress = {}\n",
    "    for dist, taluks in dist_taluka_dict.items():\n",
    "        agg_stress = 0\n",
    "        for taluk in taluks:\n",
    "            try:\n",
    "                agg_stress = agg_stress + taluka_stress_dict[taluk.lower()]\n",
    "            except(KeyError):\n",
    "                pass\n",
    "        dist_stress[dist] = agg_stress/len(taluks)\n",
    "    return dist_stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp = pd.read_excel('../input_files/3d_Impact_scaled.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_Impact_HM = pd.DataFrame()\n",
    "aggregate_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================FOR HM -20%==============================================================\n",
    "# Getting the graph ready for HM -20%\n",
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized IMR (Kuchha Houses-20%)\", \"Normalized MMR (HM -20%)\", \"Normalized PAW (HM-20%)\")\n",
    "\n",
    "# These dict have the names vs Impact Score for HM -20%\n",
    "IMR_impact_dict = dict(zip(df_imp[\"Taluka\"],df_imp[\"IMR HI -20% | Impact\"]))\n",
    "MMR_impact_dict = dict(zip(df_imp[\"Taluka\"],df_imp[\"MMR HI -20% | Impact\"]))\n",
    "PAW_impact_dict = dict(zip(df_imp[\"Taluka\"],df_imp[\"PAW HI -20% | Impact\"]))\n",
    "\n",
    "# Here, we are just converting the taluka names in these dictionaries to lower case\n",
    "IMR_impact_dict= dict((k.lower(), v) for k, v in IMR_impact_dict.items())\n",
    "MMR_impact_dict= dict((k.lower(), v) for k, v in MMR_impact_dict.items())\n",
    "PAW_impact_dict= dict((k.lower(), v) for k, v in PAW_impact_dict.items())\n",
    "\n",
    "# Aggregating both the impacts\n",
    "aggregate_IMR_Impact = aggregate(IMR_impact_dict)\n",
    "aggregate_MMR_Impact = aggregate(MMR_impact_dict)\n",
    "aggregate_PAW_Impact = aggregate(PAW_impact_dict)\n",
    "\n",
    "# Aggregating the stress\n",
    "aggregate_Stress = aggregate(get_node_stress(G))\n",
    "\n",
    "# Putting it in a temp df\n",
    "temp2_df = pd.DataFrame.from_dict([aggregate_IMR_Impact, aggregate_MMR_Impact, aggregate_PAW_Impact, aggregate_Stress])\n",
    "aI_df = temp2_df.T\n",
    "aI_df = temp2_df.transpose()\n",
    "aI_df.rename(columns = {0:'IMPACT_SCORE_IMR (HM - 20%)', 1:'IMPACT_SCORE_MMR (HM - 20%)', 2:'IMPACT_SCORE_PAW (HM - 20%)', 3:'STRESS_SCORE (HM -20%)'}, inplace = True)\n",
    "aggregate_df = aI_df\n",
    "\n",
    "# Now, we are converting them into a dataframe and making them Taluka, Impact, Stress\n",
    "combined_IMR = pd.DataFrame.from_dict([IMR_impact_dict, get_node_stress(G)])\n",
    "combined_MMR = pd.DataFrame.from_dict([MMR_impact_dict, get_node_stress(G)])\n",
    "combined_PAW = pd.DataFrame.from_dict([PAW_impact_dict, get_node_stress(G)])\n",
    "\n",
    "# Now, we are taking a transpose, so that we get it in column form\n",
    "trdIMR = combined_IMR.T\n",
    "trdMMR = combined_MMR.T\n",
    "trdPAW = combined_PAW.T\n",
    "trdIMR = combined_IMR.transpose()\n",
    "trdMMR = combined_MMR.transpose()\n",
    "trdPAW = combined_PAW.transpose()\n",
    "\n",
    "# Renaming the columns\n",
    "trdIMR.rename(columns = {0:'IMPACT_SCORE_IMR (HM - 20%)', 1:'STRESS_SCORE (HM - 20%)'}, inplace = True)\n",
    "trdMMR.rename(columns = {0:'IMPACT_SCORE_MMR (HM - 20%)', 1:'STRESS_SCORE (HM - 20%)'}, inplace = True)\n",
    "trdPAW.rename(columns = {0:'IMPACT_SCORE_PAW (HM - 20%)', 1:'STRESS_SCORE (HM - 20%)'}, inplace = True)\n",
    "\n",
    "# Filling the dataframe\n",
    "combined_Impact_HM = trdIMR\n",
    "combined_Impact_HM['IMPACT_SCORE_MMR (HM - 20%)'] = trdMMR['IMPACT_SCORE_MMR (HM - 20%)']\n",
    "combined_Impact_HM['IMPACT_SCORE_PAW (HM - 20%)'] = trdPAW['IMPACT_SCORE_PAW (HM - 20%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================FOR HM -10%==============================================================\n",
    "# Getting the graph ready for HM -10%\n",
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized IMR (Kuchha Houses-10%)\", \"Normalized MMR (HM -10%)\", \"Normalized PAW (HM-10%)\")\n",
    "\n",
    "# These dict have the names vs Impact Score for HM -10%\n",
    "IMR_impact_dict = dict(zip(df_imp[\"Taluka\"],df_imp[\"IMR HI -10% | Impact\"]))\n",
    "MMR_impact_dict = dict(zip(df_imp[\"Taluka\"],df_imp[\"MMR HI -10% | Impact\"]))\n",
    "PAW_impact_dict = dict(zip(df_imp[\"Taluka\"],df_imp[\"PAW HI -10% | Impact\"]))\n",
    "\n",
    "# Here, we are just converting the taluka names in these dictionaries to lower case\n",
    "IMR_impact_dict= dict((k.lower(), v) for k, v in IMR_impact_dict.items())\n",
    "MMR_impact_dict= dict((k.lower(), v) for k, v in MMR_impact_dict.items())\n",
    "PAW_impact_dict= dict((k.lower(), v) for k, v in PAW_impact_dict.items())\n",
    "\n",
    "# Aggregating both the impacts\n",
    "aggregate_IMR_Impact = aggregate(IMR_impact_dict)\n",
    "aggregate_MMR_Impact = aggregate(MMR_impact_dict)\n",
    "aggregate_PAW_Impact = aggregate(PAW_impact_dict)\n",
    "\n",
    "# Aggregating the stress\n",
    "aggregate_Stress = aggregate(get_node_stress(G))\n",
    "\n",
    "# Putting it in a temp df\n",
    "temp2_df = pd.DataFrame.from_dict([aggregate_IMR_Impact, aggregate_MMR_Impact, aggregate_PAW_Impact, aggregate_Stress])\n",
    "aI_df = temp2_df.T\n",
    "aI_df = temp2_df.transpose()\n",
    "aI_df.rename(columns = {0:'IMPACT_SCORE_IMR (HM - 10%)', 1:'IMPACT_SCORE_MMR (HM - 10%)', 2:'IMPACT_SCORE_PAW (HM - 10%)', 3:'STRESS_SCORE (HM -10%)'}, inplace = True)\n",
    "aggregate_df['IMPACT_SCORE_IMR (HM - 10%)'] = aI_df['IMPACT_SCORE_IMR (HM - 10%)']\n",
    "aggregate_df['IMPACT_SCORE_MMR (HM - 10%)'] = aI_df['IMPACT_SCORE_MMR (HM - 10%)']\n",
    "aggregate_df['IMPACT_SCORE_PAW (HM - 10%)'] = aI_df['IMPACT_SCORE_PAW (HM - 10%)']\n",
    "aggregate_df['STRESS_SCORE (HM - 10%)'] = aI_df['STRESS_SCORE (HM -10%)']\n",
    "\n",
    "# Now, we are converting them into a dataframe and making them Taluka, Impact, Stress\n",
    "combined_IMR = pd.DataFrame.from_dict([IMR_impact_dict, get_node_stress(G)])\n",
    "combined_MMR = pd.DataFrame.from_dict([MMR_impact_dict, get_node_stress(G)])\n",
    "combined_PAW = pd.DataFrame.from_dict([PAW_impact_dict, get_node_stress(G)])\n",
    "\n",
    "# Now, we are taking a transpose, so that we get it in column form\n",
    "trdIMR = combined_IMR.T\n",
    "trdMMR = combined_MMR.T\n",
    "trdPAW = combined_PAW.T\n",
    "trdIMR = combined_IMR.transpose()\n",
    "trdMMR = combined_MMR.transpose()\n",
    "trdPAW = combined_PAW.transpose()\n",
    "\n",
    "# Renaming the columns\n",
    "trdIMR.rename(columns = {0:'IMPACT_SCORE_IMR (HM - 10%)', 1:'STRESS_SCORE (HM -10%)'}, inplace = True)\n",
    "trdMMR.rename(columns = {0:'IMPACT_SCORE_MMR (HM - 10%)', 1:'STRESS_SCORE (HM -10%)'}, inplace = True)\n",
    "trdPAW.rename(columns = {0:'IMPACT_SCORE_PAW (HM - 10%)', 1:'STRESS_SCORE (HM -10%)'}, inplace = True)\n",
    "\n",
    "# Filling the dataframe\n",
    "combined_Impact_HM['STRESS_SCORE (HM - 10%)'] = trdPAW['STRESS_SCORE (HM -10%)']\n",
    "combined_Impact_HM['IMPACT_SCORE_IMR (HM - 10%)'] = trdIMR['IMPACT_SCORE_IMR (HM - 10%)']\n",
    "combined_Impact_HM['IMPACT_SCORE_MMR (HM - 10%)'] = trdMMR['IMPACT_SCORE_MMR (HM - 10%)']\n",
    "combined_Impact_HM['IMPACT_SCORE_PAW (HM - 10%)'] = trdPAW['IMPACT_SCORE_PAW (HM - 10%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================FOR HM +10%==============================================================\n",
    "# Getting the graph ready for HM +10%\n",
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized IMR (Kuchha Houses+10%)\", \"Normalized MMR (HM +10%)\", \"Normalized PAW (HM+10%)\")\n",
    "\n",
    "# These dict have the names vs Impact Score for HM +10%\n",
    "IMR_impact_dict = dict(zip(df_imp[\"Taluka\"],df_imp[\"IMR HI +10% | Impact\"]))\n",
    "MMR_impact_dict = dict(zip(df_imp[\"Taluka\"],df_imp[\"MMR HI +10% | Impact\"]))\n",
    "PAW_impact_dict = dict(zip(df_imp[\"Taluka\"],df_imp[\"PAW HI +10% | Impact\"]))\n",
    "\n",
    "# Here, we are just converting the taluka names in these dictionaries to lower case\n",
    "IMR_impact_dict= dict((k.lower(), v) for k, v in IMR_impact_dict.items())\n",
    "MMR_impact_dict= dict((k.lower(), v) for k, v in MMR_impact_dict.items())\n",
    "PAW_impact_dict= dict((k.lower(), v) for k, v in PAW_impact_dict.items())\n",
    "\n",
    "# Aggregating both the impacts\n",
    "aggregate_IMR_Impact = aggregate(IMR_impact_dict)\n",
    "aggregate_MMR_Impact = aggregate(MMR_impact_dict)\n",
    "aggregate_PAW_Impact = aggregate(PAW_impact_dict)\n",
    "\n",
    "# Aggregating the stress\n",
    "aggregate_Stress = aggregate(get_node_stress(G))\n",
    "\n",
    "# Putting it in a temp df\n",
    "temp2_df = pd.DataFrame.from_dict([aggregate_IMR_Impact, aggregate_MMR_Impact, aggregate_PAW_Impact, aggregate_Stress])\n",
    "aI_df = temp2_df.T\n",
    "aI_df = temp2_df.transpose()\n",
    "aI_df.rename(columns = {0:'IMPACT_SCORE_IMR (HM + 10%)', 1:'IMPACT_SCORE_MMR (HM + 10%)', 2:'IMPACT_SCORE_PAW (HM + 10%)', 3:'STRESS_SCORE (HM +10%)'}, inplace = True)\n",
    "aggregate_df['IMPACT_SCORE_IMR (HM + 10%)'] = aI_df['IMPACT_SCORE_IMR (HM + 10%)']\n",
    "aggregate_df['IMPACT_SCORE_MMR (HM + 10%)'] = aI_df['IMPACT_SCORE_MMR (HM + 10%)']\n",
    "aggregate_df['IMPACT_SCORE_PAW (HM + 10%)'] = aI_df['IMPACT_SCORE_PAW (HM + 10%)']\n",
    "aggregate_df['STRESS_SCORE (HM + 10%)'] = aI_df['STRESS_SCORE (HM +10%)']\n",
    "\n",
    "# Now, we are converting them into a dataframe and making them Taluka, Impact, Stress\n",
    "combined_IMR = pd.DataFrame.from_dict([IMR_impact_dict, get_node_stress(G)])\n",
    "combined_MMR = pd.DataFrame.from_dict([MMR_impact_dict, get_node_stress(G)])\n",
    "combined_PAW = pd.DataFrame.from_dict([PAW_impact_dict, get_node_stress(G)])\n",
    "\n",
    "# Now, we are taking a transpose, so that we get it in column form\n",
    "trdIMR = combined_IMR.T\n",
    "trdMMR = combined_MMR.T\n",
    "trdPAW = combined_PAW.T\n",
    "trdIMR = combined_IMR.transpose()\n",
    "trdMMR = combined_MMR.transpose()\n",
    "trdPAW = combined_PAW.transpose()\n",
    "\n",
    "# Renaming the columns\n",
    "trdIMR.rename(columns = {0:'IMPACT_SCORE_IMR (HM + 10%)', 1:'STRESS_SCORE (HM + 10%)'}, inplace = True)\n",
    "trdMMR.rename(columns = {0:'IMPACT_SCORE_MMR (HM + 10%)', 1:'STRESS_SCORE (HM + 10%)'}, inplace = True)\n",
    "trdPAW.rename(columns = {0:'IMPACT_SCORE_PAW (HM + 10%)', 1:'STRESS_SCORE (HM + 10%)'}, inplace = True)\n",
    "\n",
    "# Filling the dataframe\n",
    "combined_Impact_HM['STRESS_SCORE (HM + 10%)'] = trdPAW['STRESS_SCORE (HM + 10%)']\n",
    "combined_Impact_HM['IMPACT_SCORE_IMR (HM + 10%)'] = trdIMR['IMPACT_SCORE_IMR (HM + 10%)']\n",
    "combined_Impact_HM['IMPACT_SCORE_MMR (HM + 10%)'] = trdMMR['IMPACT_SCORE_MMR (HM + 10%)']\n",
    "combined_Impact_HM['IMPACT_SCORE_PAW (HM + 10%)'] = trdPAW['IMPACT_SCORE_PAW (HM + 10%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================FOR HM +20%==============================================================\n",
    "# Getting the graph ready for HM +20%\n",
    "init_graph_attr(G, '../input_files/IMR_Stress_AdjFile.xlsx', df, \"Normalized IMR (Kuchha Houses+20%)\", \"Normalized MMR (HM +20%)\", \"Normalized PAW (HM+20%)\")\n",
    "\n",
    "# These dict have the names vs Impact Score for HM +20%\n",
    "IMR_impact_dict = dict(zip(df_imp[\"Taluka\"],df_imp[\"IMR HI +20% | Impact\"]))\n",
    "MMR_impact_dict = dict(zip(df_imp[\"Taluka\"],df_imp[\"MMR HI +20% | Impact\"]))\n",
    "PAW_impact_dict = dict(zip(df_imp[\"Taluka\"],df_imp[\"PAW HI +20% | Impact\"]))\n",
    "MMR_impact_dict= dict((k.lower(), v) for k, v in MMR_impact_dict.items())\n",
    "PAW_impact_dict= dict((k.lower(), v) for k, v in PAW_impact_dict.items())\n",
    "\n",
    "# Here, we are just converting the taluka names in these dictionaries to lower case\n",
    "IMR_impact_dict= dict((k.lower(), v) for k, v in IMR_impact_dict.items())\n",
    "MMR_impact_dict= dict((k.lower(), v) for k, v in MMR_impact_dict.items())\n",
    "PAW_impact_dict= dict((k.lower(), v) for k, v in PAW_impact_dict.items())\n",
    "\n",
    "# Aggregating both the impacts\n",
    "aggregate_IMR_Impact = aggregate(IMR_impact_dict)\n",
    "aggregate_MMR_Impact = aggregate(MMR_impact_dict)\n",
    "aggregate_PAW_Impact = aggregate(PAW_impact_dict)\n",
    "\n",
    "# Aggregating the stress\n",
    "aggregate_Stress = aggregate(get_node_stress(G))\n",
    "\n",
    "# Putting it in a temp df\n",
    "temp2_df = pd.DataFrame.from_dict([aggregate_IMR_Impact, aggregate_MMR_Impact, aggregate_PAW_Impact, aggregate_Stress])\n",
    "aI_df = temp2_df.T\n",
    "aI_df = temp2_df.transpose()\n",
    "aI_df.rename(columns = {0:'IMPACT_SCORE_IMR (HM + 20%)', 1:'IMPACT_SCORE_MMR (HM + 20%)', 2:'IMPACT_SCORE_PAW (HM + 20%)', 3:'STRESS_SCORE (HM +20%)'}, inplace = True)\n",
    "aggregate_df['IMPACT_SCORE_IMR (HM + 20%)'] = aI_df['IMPACT_SCORE_IMR (HM + 20%)']\n",
    "aggregate_df['IMPACT_SCORE_MMR (HM + 20%)'] = aI_df['IMPACT_SCORE_MMR (HM + 20%)']\n",
    "aggregate_df['IMPACT_SCORE_PAW (HM + 20%)'] = aI_df['IMPACT_SCORE_PAW (HM + 20%)']\n",
    "aggregate_df['STRESS_SCORE (HM + 20%)'] = aI_df['STRESS_SCORE (HM +20%)']\n",
    "\n",
    "# Now, we are converting them into a dataframe and making them Taluka, Impact, Stress\n",
    "combined_IMR = pd.DataFrame.from_dict([IMR_impact_dict, get_node_stress(G)])\n",
    "combined_MMR = pd.DataFrame.from_dict([MMR_impact_dict, get_node_stress(G)])\n",
    "combined_PAW = pd.DataFrame.from_dict([PAW_impact_dict, get_node_stress(G)])\n",
    "\n",
    "# Now, we are taking a transpose, so that we get it in column form\n",
    "trdIMR = combined_IMR.T\n",
    "trdMMR = combined_MMR.T\n",
    "trdPAW = combined_PAW.T\n",
    "trdIMR = combined_IMR.transpose()\n",
    "trdMMR = combined_MMR.transpose()\n",
    "trdPAW = combined_PAW.transpose()\n",
    "\n",
    "# Renaming the columns\n",
    "trdIMR.rename(columns = {0:'IMPACT_SCORE_IMR (HM + 20%)', 1:'STRESS_SCORE (HM + 20%)'}, inplace = True)\n",
    "trdMMR.rename(columns = {0:'IMPACT_SCORE_MMR (HM + 20%)', 1:'STRESS_SCORE (HM + 20%)'}, inplace = True)\n",
    "trdPAW.rename(columns = {0:'IMPACT_SCORE_PAW (HM + 20%)', 1:'STRESS_SCORE (HM + 20%)'}, inplace = True)\n",
    "\n",
    "# Filling the dataframe\n",
    "combined_Impact_HM['STRESS_SCORE (HM + 20%)'] = trdPAW['STRESS_SCORE (HM + 20%)']\n",
    "combined_Impact_HM['IMPACT_SCORE_IMR (HM + 20%)'] = trdIMR['IMPACT_SCORE_IMR (HM + 20%)']\n",
    "combined_Impact_HM['IMPACT_SCORE_MMR (HM + 20%)'] = trdMMR['IMPACT_SCORE_MMR (HM + 20%)']\n",
    "combined_Impact_HM['IMPACT_SCORE_PAW (HM + 20%)'] = trdPAW['IMPACT_SCORE_PAW (HM + 20%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_Impact_HM.to_excel('../windows_3d/3D_stress_impact_talukas.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_df.to_excel('../windows_3d/3D_aggregate_stress.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================SCORE===================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = pd.DataFrame()\n",
    "df_score['Taluka'] = df['Taluka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "afzalpur     0.135375\n",
       "ajjampura    0.254919\n",
       "aland        0.301386\n",
       "alnavara     0.055094\n",
       "alur         0.190158\n",
       "               ...   \n",
       "yadrami      0.294742\n",
       "yalandur     0.403238\n",
       "yelahanka    0.315956\n",
       "yelburga     0.223477\n",
       "yellapur     0.288882\n",
       "Name: STRESS_SCORE (HM + 10%), Length: 226, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_Impact_HM['STRESS_SCORE (HM + 10%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissonance_score_3d(df, score_list, i):\n",
    "    str_imr = 'IMPACT_SCORE_IMR (HM '+ i + '0%)'\n",
    "    str_mmr = 'IMPACT_SCORE_IMR (HM '+ i + '0%)'\n",
    "    str_paw = 'IMPACT_SCORE_PAW (HM '+ i + '0%)'\n",
    "    str_stress = 'STRESS_SCORE (HM ' + i + '0%)'\n",
    "    for j in range(len(df)):\n",
    "        mean_impact = (df[str_imr][j] + df[str_mmr][j] + df[str_paw][j])\n",
    "        dissonance = abs(max(df[str_imr][j], df[str_mmr][j], df[str_paw][j]) - min(df[str_imr][j], df[str_mmr][j], df[str_paw][j]))\n",
    "        score = (mean_impact * (1 - df[str_stress][j])) / dissonance\n",
    "        score_list.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ['+ 2', '+ 1', '- 1', '- 2']\n",
    "for i in temp:\n",
    "    score_list = []\n",
    "    dissonance_score_3d(combined_Impact_HM, score_list, i)\n",
    "    str = 'Score | HM ' + i + '0%'\n",
    "    df_score[str] = score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score.to_excel('../windows_3d/3D_taluka_scores.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_score_df = pd.DataFrame()\n",
    "aggregated_score_df['District'] = dist_taluka_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in temp:\n",
    "    temp_dict = dict(zip(df_score['Taluka'],df_score['Score | HM ' + i + '0%']))\n",
    "    agg_dict = aggregate(temp_dict)\n",
    "    aggregated_score_df['Score | HM ' + i + '0%'] = agg_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_score_df.to_excel('../windows_3d/3D_district_scores.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
